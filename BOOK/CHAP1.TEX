\chapter{绪论}
\thispagestyle{empty}

\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}
%\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
%\shadowbox{
%\begin{tcolorbox}[arc=0mm,colback=white,colframe=darkblue,title=学习目标与要求]
%\kai\textcolor{darkblue}{1.~~了解科学计算的一般过程．}\\
%\kai\textcolor{darkblue}{2.~~了解数值计算方法的研究内容和特点．}\\
%\kai\textcolor{darkblue}{3.~~理解数值计算误差的有关概念．}\\
%\kai\textcolor{darkblue}{4.~~掌握数值计算误差的控制方法．}
%\end{tcolorbox}}
%\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}

\section{最优化问题}
\subsection{原始问题}
\subsection{拉格朗日对偶问题}

\section{梯度下降法（gradient descent）}
梯度下降法或者最速下降法（steepest descent）是求解无约束最优化问题的方法。特点是实现起来比较简单。其原理是如果函数$f(x)$在点$a$处可微且有定义，那么函数$f(x)$在$a$点沿着梯度的反方向，即$-\nabla f(a)$，下降最快。

所以，可以从一个初始值$x_0$出发，沿梯度反方向迭代的更新解。如下
$$x_{n+1} = x_n - \alpha\nabla f(x_n)$$
直到$x_n$的值不再发生变化，或者变化很小，此时，$x_n$等于或者接近$f(x)$的极小值。$\alpha$称为学习率（learning rate）。$\alpha$值过大，可能会在最小值附近振荡。$\alpha$值过小，可能学习的时间比较长。同时，$\alpha$值的选取可以是预先设定的固定值，也可以是根据解更新的情况变化的值。

梯度下降法的一个问题在于，能否得到最优解取决于初始值的选取。
\section{牛顿法和拟牛顿法}
\subsection{牛顿法（Newton Method）}
牛顿法，或牛顿-拉夫逊法（Newton-Raphson Method）也是求解无约束优化问题的常用方法。牛顿法是二阶收敛的算法（不仅考虑梯度方向，同时考虑梯度的梯度），而梯度下降法是一阶收敛的，因此牛顿法的收敛速度比梯度下降快。换句话说，牛顿法用二次曲面来拟合当前所在位置的局部曲面，然后按照曲率最大的方向下降。而梯度法是用一个平面去拟合局部曲面，然后按照平面的法向量的方向下降。但是一次迭代的代价比较高，因为需要计算矩阵的逆。

考虑无约束的最优化问题
$$\mathop{\min}\limits_{x\in \mathbb{R}^n} f(x)$$
假设$f(x)$有二阶连续偏导数，且设第$k$次迭代的解为$x_k$，将$f(x)$在点$x_k$处进行二阶泰勒展开
$$f(x)=f(x_k)+(x-x_k)f^{\prime}(x_k)+\frac{1}{2}(x-x_k)^2f^{\prime\prime}(x_k)$$
函数$f(x)$在下次迭代点$x_{k+1}$处取得极值的必要条件是$f^{\prime}(x_{k+1})=0$，即
$$f^{\prime}(x)|_{x_{k+1}}=f^{\prime}(x_k)+(x_{k+1}-x_{k})f^{\prime\prime}(x_{k})=0$$
解上式得到
$$x_{k+1}=x_{k}-\frac{f^{\prime}(x_{k})}{f^{\prime\prime}(x_{k})}$$
迭代停止的条件可以设定为$f^{\prime}(x_{k})<\epsilon$。

当$x$是向量的时候，其一阶导数要修改成梯度的形式，二阶导数修改成其Hessian矩阵，即
$$f(x)=f(x_k)+(x-x_k)\nabla_{x}f(x_k)+\frac{1}{2}(x-x_k)^TH(x_k)(x-x_{k})$$
当$H(x_{k})$是正定矩阵时，$f(x)$的极值为极小值。其更新公式是
$$x_{k+1}=x_{k}-H^{-1}(x_{k})\nabla_{x}f(x_{k})$$
迭代终止的条件$\nabla_xf(x_{k})<\epsilon$.

当初始点离极值点较远时，牛顿法可能不收敛，因为此时由Hessian矩阵的逆矩阵和梯度规定的牛顿方向不一定是下降方向。
\subsection{拟牛顿法（Quasi Newton Method）}
牛顿法中Hessian矩阵求逆比较复杂，所以实际中会考虑用一个近似的正定对称矩阵来代替Hessian矩阵，这就是拟牛顿法。不同的替代方法形成了不同的拟牛顿法。

\subsubsection{拟牛顿条件}
首先将函数$f(x)$在第$k$次迭代点处泰勒展开
$$f(x)=f(x_k)+(x-x_k)\nabla_{x}f(x_k)+\frac{1}{2}(x-x_k)^TH(x_k)(x-x_{k})$$
求其此时的梯度
$$\nabla f(x) = \nabla f(x_{k}) + (x-x_{k})H(x_{k})$$
令$x=x_{k+1}$，
$$\nabla f(x_{k+1}) - \nabla f(x_{k}) = (x_{k+1}-x_{k})H(x_{k})$$
左边是梯度的变化，右边$(x_{k+1}-x{k})$是自变量的变化，分别记为
\begin{align}
g_{k} &= \nabla f(x_{k+1}) - \nabla f(x_{k}) \notag \\
\delta_{k} &= x_{k+1}-x_{k} \notag
\end{align}
得到拟牛顿条件
\begin{equation}
g_{k} = H_{k}\delta_{k}
\end{equation}
或
\begin{equation}
H_{k}^{-1}g_{k} = \delta_{k}
\end{equation}

拟牛顿法选择$G_{k} \approx H_{k}^{-1}$，或者$B_{k} \approx H_{k}$。而且，尽量使得近似矩阵的更新方式为迭代更新
$$G_{k+1}=G_{k}+\Delta G_{k}$$
且$G_{k}$满足拟牛顿条件
\begin{equation}
G_{k+1}g_{k}=\delta_{k}
\end{equation}
一般令$G_0 = I$为单位阵，所以只需要找到$\Delta G_{k}$即可。
\subsubsection{DFP算法}
DFP算法最早由William C. Davidon于1959年提出，后由Roger Fletcher和Michael J.D. Powell发展和完善。DFP算法令校正矩阵为
$$\Delta G_{k} = \frac{\delta_{k}\delta_{k}^{T}}{\delta_{k}^{T}g_{k}}-\frac{G_{k}g_{k}g_{k}^{T}G_{k}}{g_{k}^{T}G_{k}g_{k}}$$
构造过程主要的规则在于保证如果初始矩阵$G_{0}$正定，那么每个$G_{k}$都是正定。

DFP算法

输入：目标函数$f(x)$，梯度$\nabla f(x)$，精度要求$\epsilon$

输出：$f(x)$的极小值点$x^{\star}$ \\
（1）选定初始点$x_0$，取$G_0$为正定对称矩阵（单位阵），置$k=0$ \\
（2）计算$\nabla f(x_k)$，若$\| \nabla f(x_k) \|< \epsilon$，令$x^{\star}=x_k$，停止计算\\
（3）令$p_k=-G_k\nabla f(x_k)$\\
（4）一维搜索：求$\lambda_k$使得
$$f(x_k+\lambda_k p_k)=\mathop{\min}\limits_{\lambda \ge 0}f(x_k+\lambda p_k)$$
（5）令$x_{k+1}=x_{k}+\lambda_k p_k$\\
（6）计算$\nabla f(x_{k+1})$，若$\| \nabla f(x_{k+1}) \|< \epsilon$，令$x^{\star}=x_{k+1}$，停止计算；否则，按$G_{k+1}=G_{k}+\Delta G_{k}$计算$G_{k+1}$\\
（7）置$k=k+1$，转（3）

\subsubsection{BFGS算法}
BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）是最流行的拟牛顿法。DFP用一个正定矩阵来近似$H^{-1}$，BFGS用一个正定矩阵来近似$H$，其表示如下
\begin{align}
B_{k+1} &= B_k + \Delta B_k\\
\Delta B_k & = \frac{g_kg_k^T}{g_k^T\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}
\end{align}
构造的核心也是满足如果初始矩阵$B_0$是正定的，那么迭代过程中的每一个$B_k$都是正定的。

BFGS算法

输入：目标函数$f(x)$，梯度$\nabla f(x)$，精度$\epsilon$

输出：$f(x)$的极小值点$x^{\star}$\\
（1）选定初始点$x_0$，取$B_0$为正定对称矩阵（单位阵），置$k=0$ \\
（2）计算$\nabla f(x_k)$，若$\| \nabla f(x_k) \|< \epsilon$，令$x^{\star}=x_k$，停止计算\\
（3）由$B_kp_k=-\nabla f(x_k)$求出$p_k$\\
（4）一维搜索：求$\lambda_k$使得
$$f(x_k+\lambda_k p_k)=\mathop{\min}\limits_{\lambda \ge 0}f(x_k+\lambda p_k)$$
（5）令$x_{k+1}=x_{k}+\lambda_k p_k$\\
（6）计算$\nabla f(x_{k+1})$，若$\| \nabla f(x_{k+1}) \|< \epsilon$，令$x^{\star}=x_{k+1}$，停止计算；否则，按$B_{k+1}=B_{k}+\Delta B_{k}$计算$B_{k+1}$\\
（7）置$k=k+1$，转（3）

令$G_k=B_k^{-1}$，对$B_{k+1}=B_k+\Delta B_k$运用两次Sherman-Morrison公式有
$$G_{k+1}=(I-\frac{\delta_k g_k^T}{\delta_k^T g_k})G_k(I-\frac{\delta_k g_k^T}{\delta_k^T g_k})^T+\frac{\delta_k g_k^T}{\delta_k^T g_k}$$
称为BFGS算法关于$G_k$的迭代公式。

Sherman-Morrison公式：假设$A$是$n$阶可逆矩阵，$u$，$v$是$n$维向量，且$A+uv^T$也是可逆矩阵，则
$$(A+uv^T)^{-1}=A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}$$

记DFP关于$G_k$的迭代公式得到的$G_{k+1}$为$G^{DFP}$，由BFGS得到的记为$G^{BFGS}$，它们都满足拟牛顿公式，且其线性组合
$$G_{k+1}=\alpha G^{DFP} + (1-\alpha)G^{BFGS}$$
也满足拟牛顿条件，而且是正定是。其中$0 \le \alpha \le 1$。这样得到的一类拟牛顿法称为Broyden类算法。

实际在使用的时候使用的是LBFGS算法（Limited-memory-BFGS）。有很成熟的开源实现，直接调用即可。
\section{矩阵分析（matrix analysis）}
\subsection{迹(Trace)和导数(matrix derivatives)}
令$f:\mathbb{R}^{m\times n}\mapsto \mathbb{R}$表示将$m\times n$（$m$-by-$n$）矩阵映射为实数的函数。定义$f$对矩阵$\bm{A}$的导数
$$\nabla_{\bm{A}}f(\bm{A})=\begin{pmatrix}
                            \noalign{\vskip2pt}
                            \frac{\partial f(\bm{A})}{\partial a_{11}} & \cdots &  \frac{\partial f(\bm{A})}{\partial a_{1n}}\\
                            \noalign{\vskip4pt}
                            \vdots & \ddots &\vdots \\
                            \noalign{\vskip8pt}
                            \frac{\partial f(\bm{A})}{\partial a_{n1}} & \cdots & \frac{\partial f(\bm{A})}{\partial a_{nn}}\\
                            \noalign{\vskip2pt}
                            \end{pmatrix}
$$


矩阵的迹（trace）表示的是矩阵的对角元素的和，
$$tr\bm{A}=\sum_{i=1}^{n}a_{ii}$$
假设$A$，$B$，$C$，$D$均是方阵
\begin{equation}
trABCD=trDABC=trCDAB=trBCDA
\end{equation}
循环将最右边矩阵放到最左边。假设$a$是实数
\begin{align}
trA &= trA^T\\
tr(A+B) &= trA+trB\\
traA&=atrA
\end{align}
下面的一些公式出自Andrew Ng的机器学习讲义，这里证明一下。
\begin{align}
\nabla_AtrAB &= B^T\\
\nabla_{A^T}f(A) &= (\nabla_Af(A))^T\\
\nabla_{A}trABA^TC &= CAB+C^TAB^T\\
\nabla_{A}|A|&=|A|(A^{-1})^T
\end{align}
\proof
(1)\qquad $(\nabla_AtrAB)_{ij}=\frac{\partial trAB}{\partial a_{ij}}=\frac{\partial \sum_m\sum_ka_{mk}b_{km}}{\partial a_{ij}}$，只有当$m=i,k=j$时才有$a_{ij}$的系数，所以$(\nabla_AtrAB)_{ij}=b_{ji}$，即证。

(2)\qquad $(\nabla_{A^T}f(A))_{ij}=\frac{\partial f(A)}{\partial a_{ji}}$，即证。

(3)\qquad $trABA^TC=\sum_m\sum_k\sum_t\sum_s a_{mk}b_{kt}a_{st}c_{sm}$，所以
\begin{align}
(\nabla_{A}trABA^TC)_{ij} &= \frac{\partial \sum_m\sum_k\sum_t\sum_s a_{mk}b_{kt}a_{st}c_{sm}}{\partial a_{ij}} \notag \\
&=\sum_m\sum_k\sum_t\sum_s \frac{\partial a_{mk}}{\partial a_{ij}}b_{kt}a_{st}c_{sm}+
\sum_m\sum_k\sum_t\sum_s a_{mk}b_{kt}\frac{\partial a_{st}}{\partial a_{ij}}c_{sm} \notag
\end{align}
左边，令$m=i,k=j$，右边，令$s=i, t=j$，
\begin{align}
(\nabla_{A}trABA^TC)_{ij}
&=\sum_t\sum_s b_{jt}a_{st}c_{si}+\sum_m\sum_k a_{mk}b_{kj}c_{im} \notag\\
&=\sum_t\sum_s b_{jt}a_{st}c_{si}+\sum_m\sum_k c_{im}a_{mk}b_{kj} \notag\\
&=(BA^TC)_{ji}+(CAB)_{ij} \notag\\
&=(C^TAB^T+CAB)_{ij}\notag
\end{align}

\section{常用不等式}
\subsection{柯西不等式（Cauchy Inequality）}
柯西不等式，又称柯西-施瓦茨不等式（Cauchy-Schwarz inequality）。对于一个内积空间所有向量$\bm{x}$和$\bm{y}$，
$$|\langle \bm{x},\bm{y} \rangle|^2 \le \langle\bm{x}, \bm{x} \rangle\cdot\langle\bm{y}, \bm{y}\rangle$$
其中$\langle\cdot, \cdot\rangle$表示内积（点积），当且仅当$\bm{x}$与$\bm{y}$线性相关时等式成立。

对于欧几里得空间$\mathbb{R}^2$，
$$(\sum_{i=1}^{n}x_iy_i)^2 \le (\sum_{i=1}^{n}x_i^2)(\sum_{i=1}^{n}y_i^2)$$
当且仅当$\frac{x_1}{y_1}=\frac{x_2}{y_2}=\cdots=\frac{x_n}{y_n}$时等式成立。

\subsection{赫尔德不等式（H$\ddot{o}$lder Inequality）}
赫尔德不等式揭示了$L^p$空间的相互关系。设$S$为测度空间，$1\le p,q \le \inf$，且$\frac{1}{p}+\frac{1}{q}=1$，若$f \in L^p(S)$，$g \in L^q(S)$，则$fg\in L^1(S)$，且
$$\parallel fg \parallel_1 \le \parallel f \parallel_p \parallel g \parallel_q$$


写成序列或向量的形式
$$\sum_{i=1}^{n}|a_ib_i| \le (\sum_{i=1}^{n}|a_i|^p)^{\frac{1}{p}}(\sum_{i=1}^{n}|b_i|^q)^{\frac{1}{q}}$$

\subsection{闵可夫斯基不等式（Minkowski Inequality）}
闵可夫斯基不等式表明$L^p$空间是一个赋范向量空间。设$S$是一个度量空间，$f,g \in L^p(S), 1\le p \le \inf$，那么$f+g \in L^p(S)$，有
$$\parallel f+g \parallel_p \le \parallel f \parallel_p + \parallel g \parallel_p$$

写成序列或向量的形式
$$(\sum_{k=1}^n|x_k+y_k|^p)^{\frac{1}{p}} \le (\sum_{k=1}^n|x_k|^p)^{\frac{1}{p}}(\sum_{k=1}^{n}|y_k|^p)^{\frac{1}{p}}$$



