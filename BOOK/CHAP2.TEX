\chapter{回归}
\thispagestyle{empty}

\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}


%\section{模型}
给定数据集
    $$T=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})\},$$
    其中，$x^{(i)}=(x_0, x_1, \cdots, x_n) \in \mathcal{X} = \mathbb{R}^{n+1}$，$y^{(i)} \in \mathcal{Y}$, $i=0,2,\cdots, m$，且$x_0=1$ （表示截距，intercept term）。回归分析的任务是找出输入$x$ 与输出$y$之间的关系。

\section{线性回归}
假设输入与输出之间满足的关系是线性的，$theta$称为参数（parameters）或者权重（weights）
    $$y=h_{\theta}(x)={\theta}^Tx=\sum_{i=0}^{n}\theta_ix_i, \qquad \theta \in \mathbb{R}^{n+1}$$

对于这个模型，需要有一个损失函数（cost function）来表示其对训练数据的拟合程度
    $$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2.$$
这个被称为最小二乘方效用函数（least-square cost function）。

则这个问题的求解可以表述为如下的无约束最优化问题
$$\mathop{\min}\limits_\theta J(\theta)$$

\subsection{直接求解}
直接求$J(\theta)$对$\theta$的极值。首先定义设计矩阵（design matrix）$X$
\begin{align}
X&=\begin{bmatrix}(x^{(1)})^T & (x^{(2)})^T & \cdots & (x^{(m)})^T \end{bmatrix}^T  \notag \\
Y&=\begin{bmatrix}y^{(1)} & y^{(2)} & \cdots & y^{(m)} \end{bmatrix}^T  \notag
\end{align}
则有
$$
X\theta - Y = \begin{bmatrix}h_{\theta}(x^{(1)})-y^{(1)} & \cdots & h_{\theta}(x^{(m)})-y^{(m)}\end{bmatrix}^T
$$
考虑到$z^Tz=\sum_iz_i^2$，有
$$
J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

使用公式$\nabla_{A}trABA^TC = CAB+C^TAB^T$，且由于$J(\theta)$只是个实数，所以
\begin{align}
\nabla_{\theta}J(\theta)&=\nabla_{\theta}trJ(\theta) \notag\\
&=X^TX\theta-X^TY \notag
\end{align}
令$\nabla_{\theta}J(\theta)=0$得到
$$\theta=(X^TX)^{-1}X^TY$$

不过，这个公式用来直接计算$\theta$不现实，因为矩阵求逆比较麻烦，同时可能会是数值不稳定的矩阵。

\subsection{牛顿法（Newton's Method）}
已知最优化问题
$$\mathop{\min}\limits_\theta J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$
函数$J(\theta)$的Hessian矩阵是（注意$x^{(i)}$和$\theta$都是列向量）
$$H(\theta)=\begin{bmatrix}
\frac{\partial^2 J(\theta)}{\partial \theta_1 \partial \theta_1} & \frac{\partial^2 J(\theta)}{\partial \theta_1 \partial \theta_2} & \cdots & \frac{\partial^2 J(\theta)}{\partial \theta_1 \partial \theta_n}\\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 J(\theta)}{\partial \theta_n \partial \theta_1} & \frac{\partial^2 J(\theta)}{\partial \theta_n \partial \theta_2} & \cdots & \frac{\partial^2 J(\theta)}{\partial \theta_n \partial \theta_n}
\end{bmatrix}
=\sum_{i=1}^mx^{(i)}(x^{(i)})^T=X^TX$$
又，$J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)$，所以其梯度
$$\nabla_{\theta}J(\theta)=X^TX\theta-X^TY$$
代入牛顿法的迭代公式
$$\theta_{n+1}=\theta_{n}-(X^TX)^{-1}(X^TX\theta_n-X^TY)=(X^TX)^{-1}X^TY$$


\subsection{批处理梯度下降法（batch gradient descent）}
$J(\theta)$对$\theta$的梯度
$$\frac{\partial J(\theta)}{\partial \theta}=\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}$$
所以，更新公式为
\begin{align}
\theta_{n+1} &= \theta_n - \alpha\frac{\partial J(\theta)}{\partial \theta} \notag \\
&=\theta_n - \alpha \sum_{i=1}^{m}(h_{\theta_n}(x^{(i)})-y^{(i)})x^{(i)} \notag
\end{align}
这个公式更新时每次都需要全部的训练数据集，所以称之为批处理梯度下降法。当数据集比较大时，进行一次更新就比较耗费时间。

\subsection{随机梯度下降法（stochastic gradient descent）}
随机梯度下降法，也称增量梯度下降（incremental gradient descent），一次使用一条训练数据来更新参数。算法如下
$$\theta := \theta - \alpha(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}, i=1,2,\cdots, m$$
随机梯度下降法更新的速度比梯度下降法快，但可能收敛不到最优值，不过通过调节学习率可以使得算法得到较好的解。而且随机梯度下降法可以用作在线学习的算法。

\section{局部加权线性回归（LWR）}
线性回归的方法是参数学习算法（parametric learning algorithm），其参数的个数，即特征的个数是固定的，一旦算法学习完成，训练数据集就不再对参数产生影响。但是，当选取的参数过多时，可能存在过拟合问题，而当选取的参数过少时，存在欠拟合问题。局部加权线性回归（local weighted linear regression， LWR）是一种非参学习算法（non-parametric learning algorithm），其参数是随着预测点的不同而发生变化的，每有一个新的预测点，就需要整个训练数据集重新参与学习。所谓局部，是因为目标函数的逼近仅仅根据查询点附近的数据。所谓加权，是因为每个训练样例的贡献都是由它与查询点间的距离加权的。而回归是指数值逼近的方法。

线性回归的优化目标是
$$\mathop{\min}\limits_\theta \sum_{i=1}^{m}(\theta^T x^{(i)}-y^{(i)})^2$$
而LWR的优化目标是在上述公式上增加一个距离乘法项
$$\mathop{\min}\limits_\theta \sum_{i=1}^{m}w^{(i)}(\theta^T x^{(i)}-y^{(i)})^2$$
一个相对标准的权重选择是
$$w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2\tau^2})$$
其中，$x$是要预测的输入数据，$\tau$称为带宽（bandwidth）。权重项使得离输入数据$x$越近的点影响越大。

非参数学习算法在局部预测能力上有时要比参数学习算法好，但是缺点是每次做预测都要重新学习，耗费时间空间。

\section{Logistic回归}
\subsection{二项逻辑斯谛回归}
逻辑斯谛回归（logistic regression）是分类模型，跟线性回归模型有相同的输入，但是其输出是离散值。但是，之所以叫“回归”是因为它依然采用线性回归的算法来预测$x$与$y$之间的关系。也因此，需要限制模型中$y$值的输出，令
$$h_{\theta}(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$
其中
$$g(z)=\frac{1}{e^{-z}}$$
称为逻辑斯谛函数（logistic function），或者S形曲线（sigmoid curve）。

为了推导方便，首先给出$g(z)$的导数
\[g^{\prime}(z)=g(z)(1-g(z))\]

设
\begin{align}
P(y=1|x;\theta) &= h_{\theta}(x) \notag \\
P(y=0|x;\theta) &= 1-h_{\theta}(x) \notag
\end{align}
或者简写成
\[p(y|x;\theta)=(h_{\theta}(x))^{y}(1-h_{\theta}(x))^{1-y}\]
其对数似然函数
\begin{align}
l(\theta) &=\log\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)=\sum_{i=1}^{m}(y^{(i)}\log h_{\theta}(x^{(i)})+(1-y^{(i)})\log (1-h_{\theta}(x^{(i)})))\notag \\
&=\sum_{i=1}^{m}(y^{(i)}h_{\theta}(x^{(i)})+\log (1-h_{\theta}(x^{(i)})))\notag
\end{align}
其中，$m$是训练样本的个数。对$\theta$求导，得
$$\frac{\partial l(\theta)}{\partial \theta}=\sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}$$
则对于单条训练样本的更新公式是
$$\theta := \theta + \alpha (y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}$$
注意，之所以是$+$号，是因为所求的最优化问题是似然函数的最大化，也即按照梯度的方向增加。

这个更新公式跟LMS（least mean squares）更新准则一样。也即，更新的幅度由学习率以及特征向量和误差的乘积决定。这是由梯度下降法所决定的。

\subsection{多项逻辑斯谛回归}
逻辑斯谛回归其实是建立了逻辑斯谛概率模型之后依据似然函数最大化准备建立的回归模型，其学习算法一般是梯度下降或者牛顿法。多项逻辑斯谛回归的概率模型可以表述为：如果离散随机变量$Y$的取值集合是$\{1,2,\cdots, K\}$，那么多项逻辑斯谛回归模型是
\begin{align}
P(Y=k|x;\theta)&=\frac{e^{\theta^Tx}}{1+\sum_{i=1}^{K-1}e^{\theta^Tx}},k=1,2,\cdots, K-1 \\
P(Y=K|x;\theta)&=\frac{1}{1+\sum_{i=1}^{K-1}e^{\theta^Tx}},k=K
\end{align}
其中，$x,\theta \in \mathbb{R}^{n+1}$.

\section{广义线性模型}
广义线性模型（Generalized Linear Models, GLMs）是线性回归（linear regression）和逻辑斯谛回归（logistic regression）的统一表述。
\subsection{指数分布族（the exponential family）}
首先定义指数分布族
\begin{equation}
p(y;\eta)=b(y)e^{\eta^T T(y)-a(\eta)}
\end{equation}
其中，$\eta$称为分布的自然参数（natural parameter）或者标准参数（canonical parameter），$\eta^T$是取$\eta$的转置的意思，当$\eta$是标量（scalar）的时候，$\eta^T=\eta$，当$\eta$是向量（vector）的时候，上式就是向量的乘积，$T(y)$ 是充分统计量（sufficient statistic），通常令$T(y)=y$，$a(\eta)$称为对数分割函数（log partition function）。注意到$e^{-a(\eta)}$ 实际上起到的是归一化的作用，即保证分布$p(y;\eta)$对$y$的和或积分是$1$。 固定$T,a,b$就得到了以$\eta$为参数的分布族。下面将伯努利分布和高斯分布表示成指数族的形式。

伯努利分布（Bernoulli distribution）的形式为
\begin{equation}
p(y;\phi)=\phi^{y}(1-\phi)^{1-y}
\end{equation}
表示的是一次Bernoulli实验（二值）的结果。$y$表示实验成功与否：成功，则$y=1$；否则，$y=0$。

将上式表示成
\begin{align}
p(y;\phi) &= \phi^{y}(1-\phi)^{1-y} \notag \\
&= exp(y\log \phi + (1-y)\log (1-\phi)) \notag \\
&= exp((\log (\frac{\phi}{1-\phi}))y+\log (1-\phi))\notag
\end{align}
此时，令$\eta=\log \frac{\phi}{1-\phi}$就得到了Bernoulli分布的指数族形式。其中
\begin{align}
T(y) &= y \notag \\
b(\eta) &= 1 \notag \\
a(\eta) &= -\log (1-\phi)=\log (1+e^{\eta})\notag
\end{align}
注意到$\phi=\frac{1}{1+e^{-\eta}}$刚好是sigmoid函数。

高斯分布（Gaussian distribution）的一般形式
\begin{equation}
p(y;\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\mu)^2}{2\sigma^2})
\end{equation}
考虑到$\sigma$对假设函数没有影响，所以可以令$\sigma=1$
\begin{equation}
p(y;\mu)=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}(y-\mu)^2)
\end{equation}
表示为GMLs形式，有
\begin{align}
\eta &= \mu \notag \\
T(y) &= y \notag \\
a(\eta) &= \frac{\mu^2}{2}=\frac{\eta^2}{2}\notag \\
b(y) &= \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}\notag
\end{align}

如果要考虑$\sigma$的话，需要用到GLMs的更为一般的模型
\begin{equation}
p(y;\eta, \tau)=b(a,\tau)exp(\frac{\eta^TT(y)-a(\eta)}{c(\tau)})
\end{equation}
其中，$\eta=(\mu, \sigma)\in \mathbb{R}^2$，$\tau$称为离差参数（dispersion parameter），对于高斯分布来说，$c(\tau)=\sigma^2$。

\subsection{构造GLMs}
构造GLMs有三个假设：\\
1.$y|x;\theta \sim ExponentialFamily(\eta)$，即给定特征向量$x$和参数$\theta$，输出$y$符合以$\eta$为参数的指数分布族。\\
2.给定$x$，目的是要求$T(y)$的期望值，即$h(x)=E[T(y)|x]$。$T(y)$是$y$的充分统计量，一般是$T(y)=y$。\\
3.自然参数$\eta$和输入$x$之间满足线性关系：$\eta=\theta^Tx$。 

\subsubsection{最小二乘法（Ordinary Least Square）}
 考虑到目标变量（target variable）或者响应变量（response variable）$y$是连续的，假定其服从高斯分布，即$y|x;\theta \sim \mathcal{N}(\mu, \sigma^2)$。注意高斯分布的期望是$\mu$，标准差是$\sigma$。 此时，$T(y)=y$，所以
 \begin{align}
 h_\theta(x) &= E[T(y)|x;\theta]=E[y|x;\theta] \notag \\
 &=\mu \notag \\
 &=\eta \notag \\
 &=\theta^Tx \notag
 \end{align}
 第一行基于假设2，第二行基于高斯分布的特性，第三行基于假设1，即高斯分布作为指数分布族的一个特例，第四行基于假设3.
 
 \subsubsection{Logistic Regression} 
 LogisticRegression的输出只有$y\in \{0,1\}$两个值，所以考虑输出满足伯努利分布，即$y|x;\theta \sim Bernoulli(\phi)$，而伯努利分布的期望$E[y|x;\theta]=\phi$，所以有
 \begin{align}
  h_\theta(x) &= E[T(y)|x;\theta]=E[y|x;\theta] \notag \\
 &=\phi \notag \\
 &=\frac{1}{1+e^{-\eta}} \notag \\
 &=\frac{1}{1+e^{-\theta^Tx}} \notag
 \end{align} 
 这样就得到了logistic函数$\frac{1}{1+e^{-z}}$。
 
 \subsubsection{Softmax Regression}
 考虑一个多类分类问题，假设$y \in \{1,2,\cdots, k\}$，此时可以用多项式分布对$y$进行建模。最终可以得到多元逻辑斯谛回归模型。
 
 \subsection{总结}
 从上面可以看出，广义线性模型（GLMs）的作用其实是提供一个统一的方法来实现训练模型的选择。其流程如下所示
 \begin{align}
 y|x;\theta \sim GLMs &\rightarrow \text{模型（logistic回归、softmax回归及最小二乘法等）}  \notag \\
 &\rightarrow \text{由likelihood函数建立最优化模型} \notag \\
 &\rightarrow \text{参数求解（梯度法、牛顿法等）} \notag
 \end{align}