\chapter{绪论}
\thispagestyle{empty}

\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}
%\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
%\shadowbox{
%\begin{tcolorbox}[arc=0mm,colback=white,colframe=darkblue,title=学习目标与要求]
%\kai\textcolor{darkblue}{1.~~了解科学计算的一般过程．}\\
%\kai\textcolor{darkblue}{2.~~了解数值计算方法的研究内容和特点．}\\
%\kai\textcolor{darkblue}{3.~~理解数值计算误差的有关概念．}\\
%\kai\textcolor{darkblue}{4.~~掌握数值计算误差的控制方法．}
%\end{tcolorbox}}
%\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}

\section{最优化问题}
\subsection{原始问题}
\subsection{拉格朗日对偶问题}

\section{梯度下降法（gradient descent）}
梯度下降法或者最速下降法（steepest descent）是求解无约束最优化问题的方法。特点是实现起来比较简单。其原理是如果函数$f(x)$在点$a$处可微且有定义，那么函数$f(x)$在$a$点沿着梯度的反方向，即$-\nabla f(a)$，下降最快。

所以，可以从一个初始值$x_0$出发，沿梯度反方向迭代的更新解。如下
$$x_{n+1} = x_n - \alpha\nabla f(x_n)$$
直到$x_n$的值不再发生变化，或者变化很小，此时，$x_n$等于或者接近$f(x)$的极小值。$\alpha$称为学习率（learning rate）。$\alpha$值过大，可能会在最小值附近振荡。$\alpha$值过小，可能学习的时间比较长。同时，$\alpha$值的选取可以是预先设定的固定值，也可以是根据解更新的情况变化的值。

梯度下降法的一个问题在于，能否得到最优解取决于初始值的选取。
\section{牛顿法和拟牛顿法}
\subsection{牛顿法（Newton Method）}
牛顿法，或牛顿-拉夫逊法（Newton-Raphson Method）也是求解无约束优化问题的常用方法。牛顿法是二阶收敛的算法（不仅考虑梯度方向，同时考虑梯度的梯度），而梯度下降法是一阶收敛的，因此牛顿法的收敛速度比梯度下降快。换句话说，牛顿法用二次曲面来拟合当前所在位置的局部曲面，然后按照曲率最大的方向下降。而梯度法是用一个平面去拟合局部曲面，然后按照平面的法向量的方向下降。但是一次迭代的代价比较高，因为需要计算矩阵的逆。

考虑无约束的最优化问题
$$\mathop{\min}\limits_{x\in \mathbb{R}^n} f(x)$$
假设$f(x)$有二阶连续偏导数，且设第$k$次迭代的解为$x_k$，将$f(x)$在点$x_k$处进行二阶泰勒展开
$$f(x)=f(x_k)+(x-x_k)f^{\prime}(x_k)+\frac{1}{2}(x-x_k)^2f^{\prime\prime}(x_k)$$
函数$f(x)$在下次迭代点$x_{k+1}$处取得极值的必要条件是$f^{\prime}(x_{k+1})=0$，即
$$f^{\prime}(x)|_{x_{k+1}}=f^{\prime}(x_k)+(x_{k+1}-x_{k})f^{\prime\prime}(x_{k})=0$$
解上式得到
$$x_{k+1}=x_{k}-\frac{f^{\prime}(x_{k})}{f^{\prime\prime}(x_{k})}$$
迭代停止的条件可以设定为$f^{\prime}(x_{k})<\epsilon$。

当$x$是向量的时候，其一阶导数要修改成梯度的形式，二阶导数修改成其Hessian矩阵，即
$$f(x)=f(x_k)+(x-x_k)\nabla_{x}f(x_k)+\frac{1}{2}(x-x_k)^TH(x_k)(x-x_{k})$$
当$H(x_{k})$是正定矩阵时，$f(x)$的极值为极小值。其更新公式是
$$x_{k+1}=x_{k}-H^{-1}(x_{k})\nabla_{x}f(x_{k})$$
迭代终止的条件$\nabla_xf(x_{k})<\epsilon$.

当初始点离极值点较远时，牛顿法可能不收敛，因为此时由Hessian矩阵的逆矩阵和梯度规定的牛顿方向不一定是下降方向。
\subsection{拟牛顿法（Quasi Newton Method）}
牛顿法中Hessian矩阵求逆比较复杂，所以实际中会考虑用一个近似的正定对称矩阵来代替Hessian矩阵，这就是拟牛顿法。不同的替代方法形成了不同的拟牛顿法。

\subsubsection{拟牛顿条件}
首先将函数$f(x)$在第$k$次迭代点处泰勒展开
$$f(x)=f(x_k)+(x-x_k)\nabla_{x}f(x_k)+\frac{1}{2}(x-x_k)^TH(x_k)(x-x_{k})$$
求其此时的梯度
$$\nabla f(x) = \nabla f(x_{k}) + (x-x_{k})H(x_{k})$$
令$x=x_{k+1}$，
$$\nabla f(x_{k+1}) - \nabla f(x_{k}) = (x_{k+1}-x_{k})H(x_{k})$$
左边是梯度的变化，右边$(x_{k+1}-x{k})$是自变量的变化，分别记为
\begin{align}
g_{k} &= \nabla f(x_{k+1}) - \nabla f(x_{k}) \notag \\
\delta_{k} &= x_{k+1}-x_{k} \notag
\end{align}
得到拟牛顿条件
\begin{equation}
g_{k} = H_{k}\delta_{k}
\end{equation}
或
\begin{equation}
H_{k}^{-1}g_{k} = \delta_{k}
\end{equation}

拟牛顿法选择$G_{k} \approx H_{k}^{-1}$，或者$B_{k} \approx H_{k}$。而且，尽量使得近似矩阵的更新方式为迭代更新
$$G_{k+1}=G_{k}+\Delta G_{k}$$
且$G_{k}$满足拟牛顿条件
\begin{equation}
G_{k+1}g_{k}=\delta_{k}
\end{equation}
一般令$G_0 = I$为单位阵，所以只需要找到$\Delta G_{k}$即可。
\subsubsection{DFP算法}
DFP算法最早由William C. Davidon于1959年提出，后由Roger Fletcher和Michael J.D. Powell发展和完善。DFP算法令校正矩阵为
$$\Delta G_{k} = \frac{\delta_{k}\delta_{k}^{T}}{\delta_{k}^{T}g_{k}}-\frac{G_{k}g_{k}g_{k}^{T}G_{k}}{g_{k}^{T}G_{k}g_{k}}$$
构造过程主要的规则在于保证如果初始矩阵$G_{0}$正定，那么每个$G_{k}$都是正定。

DFP算法

输入：目标函数$f(x)$，梯度$\nabla f(x)$，精度要求$\epsilon$

输出：$f(x)$的极小值点$x^{\star}$ \\
（1）选定初始点$x_0$，取$G_0$为正定对称矩阵（单位阵），置$k=0$ \\
（2）计算$\nabla f(x_k)$，若$\| \nabla f(x_k) \|< \epsilon$，令$x^{\star}=x_k$，停止计算\\
（3）令$p_k=-G_k\nabla f(x_k)$\\
（4）一维搜索：求$\lambda_k$使得
$$f(x_k+\lambda_k p_k)=\mathop{\min}\limits_{\lambda \ge 0}f(x_k+\lambda p_k)$$
（5）令$x_{k+1}=x_{k}+\lambda_k p_k$\\
（6）计算$\nabla f(x_{k+1})$，若$\| \nabla f(x_{k+1}) \|< \epsilon$，令$x^{\star}=x_{k+1}$，停止计算；否则，按$G_{k+1}=G_{k}+\Delta G_{k}$计算$G_{k+1}$\\
（7）置$k=k+1$，转（3）

\subsubsection{BFGS算法}
BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）是最流行的拟牛顿法。DFP用一个正定矩阵来近似$H^{-1}$，BFGS用一个正定矩阵来近似$H$，其表示如下
\begin{align}
B_{k+1} &= B_k + \Delta B_k\\
\Delta B_k & = \frac{g_kg_k^T}{g_k^T\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}
\end{align}
构造的核心也是满足如果初始矩阵$B_0$是正定的，那么迭代过程中的每一个$B_k$都是正定的。

BFGS算法

输入：目标函数$f(x)$，梯度$\nabla f(x)$，精度$\epsilon$

输出：$f(x)$的极小值点$x^{\star}$\\
（1）选定初始点$x_0$，取$B_0$为正定对称矩阵（单位阵），置$k=0$ \\
（2）计算$\nabla f(x_k)$，若$\| \nabla f(x_k) \|< \epsilon$，令$x^{\star}=x_k$，停止计算\\
（3）由$B_kp_k=-\nabla f(x_k)$求出$p_k$\\
（4）一维搜索：求$\lambda_k$使得
$$f(x_k+\lambda_k p_k)=\mathop{\min}\limits_{\lambda \ge 0}f(x_k+\lambda p_k)$$
（5）令$x_{k+1}=x_{k}+\lambda_k p_k$\\
（6）计算$\nabla f(x_{k+1})$，若$\| \nabla f(x_{k+1}) \|< \epsilon$，令$x^{\star}=x_{k+1}$，停止计算；否则，按$B_{k+1}=B_{k}+\Delta B_{k}$计算$B_{k+1}$\\
（7）置$k=k+1$，转（3）

令$G_k=B_k^{-1}$，对$B_{k+1}=B_k+\Delta B_k$运用两次Sherman-Morrison公式有
$$G_{k+1}=(I-\frac{\delta_k g_k^T}{\delta_k^T g_k})G_k(I-\frac{\delta_k g_k^T}{\delta_k^T g_k})^T+\frac{\delta_k g_k^T}{\delta_k^T g_k}$$
称为BFGS算法关于$G_k$的迭代公式。

Sherman-Morrison公式：假设$A$是$n$阶可逆矩阵，$u$，$v$是$n$维向量，且$A+uv^T$也是可逆矩阵，则
$$(A+uv^T)^{-1}=A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}$$

记DFP关于$G_k$的迭代公式得到的$G_{k+1}$为$G^{DFP}$，由BFGS得到的记为$G^{BFGS}$，它们都满足拟牛顿公式，且其线性组合
$$G_{k+1}=\alpha G^{DFP} + (1-\alpha)G^{BFGS}$$
也满足拟牛顿条件，而且是正定是。其中$0 \le \alpha \le 1$。这样得到的一类拟牛顿法称为Broyden类算法。

实际在使用的时候使用的是LBFGS算法（Limited-memory-BFGS）。有很成熟的开源实现，直接调用即可。



