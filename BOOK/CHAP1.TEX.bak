\chapter{绪论}
\thispagestyle{empty}

\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}
%\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
%\shadowbox{
%\begin{tcolorbox}[arc=0mm,colback=white,colframe=darkblue,title=学习目标与要求]
%\kai\textcolor{darkblue}{1.~~了解科学计算的一般过程．}\\
%\kai\textcolor{darkblue}{2.~~了解数值计算方法的研究内容和特点．}\\
%\kai\textcolor{darkblue}{3.~~理解数值计算误差的有关概念．}\\
%\kai\textcolor{darkblue}{4.~~掌握数值计算误差的控制方法．}
%\end{tcolorbox}}
%\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}

\section{凸函数}
\definibox{凸组合}{设$x_1,x_2,\cdots,x_m \in \mathbb{R}^n$，称$x$是$x_1,x_2,\cdots,x_m$的一个凸组合，如果存在满足$\sum_{i=1}^m \lambda_i =1$的非负的$\lambda_1,\lambda_2,\cdots,\lambda_m$使得
$$x=\sum_{i=1}^m \lambda_ix_i$$
}
\definibox{凸集}{集合$S\subset \mathbb{R}^n$是凸集的充要条件是$S$中任意若干点的任一凸组合仍属于$S$。}
\definibox{凸函数}{设$S\subset \mathbb{R}^n$是非空凸集，$f$是定义在$S$上的函数。称$f$是定义在$S$上的凸函数，如果对$\forall x_1,x_2 \in S, \lambda\in (0,1)$，都有
$$f(\lambda x_1+(1-\lambda)x_2)\le \lambda f(x_1)+(1-\lambda)f(x_2)$$
称$f$为$S$上的严格凸函数，如果当$x_1\ne x_2$时，上式中的不等号严格成立。
}

在二维平面中，凸函数是那种在函数上方所形成的点集是凸集的函数。事实上，凸函数更广泛的定义不一定是可微的。同时，所有的线性函数（linear）和仿射函数（affine）都是凸函数。仿射函数是定义为$f(x)=A^Tx+B$的函数，跟线性函数一样，只不过多了个截距项$B$。

\theorebox{凸函数的充要条件}{设$S$为非空开凸集，$f(x)$是$S$上可微函数，则$f(x)$是凸函数的充要条件是，对$\forall x^\ast \in S$，都有
$$f(x)\ge f(x^\ast)+\nabla f(x^\ast)^T(x-x^\ast), \forall x\in S$$
类似的，$f(x)$是严格凸函数的充要条件是
$$f(x)> f(x^\ast)+\nabla f(x^\ast)^T(x-x^\ast), \forall x^\ast \ne x\in S$$
}

\section{最优化问题}
\subsection{原始问题}
假设$f(x),c_i(x),h_j(x)$是定义在$\mathbb{R}^n$上的连续可微函数。则最优化问题
\begin{alignat}{2}
\mathop{\min}\limits_{x\in \mathbb{R}^n}\quad & f(x)          &{}    & \tag{C1} \label{OPPrimal}\\
\mbox{s.t.}\quad                              & c_i(x) \le 0, &\quad & i=1,2,\cdots,k \tag{C2}  \label{C2}\\
                                              & h_j(x) = 0,   &\quad & j=1,2,\cdots,l \tag{C3} \label{C3}
\end{alignat}
称为原始问题。

定义广义拉格朗日函数（generalized Lagrange function）
\[
L(x,\alpha, \beta)= f(x)+\sum_{i=1}^k \alpha_ic_i(x)+\sum_{j=1}^l \beta_jh_j(x)
\]
$\alpha_i \ge 0,\beta_j$称为拉格朗日乘子（Lagrange multipliers）。考虑等式
\[
\theta_P(x)=\mathop{\max}\limits_{\alpha,\beta;\alpha_i\ge 0}L(x,\alpha, \beta)
\]
下标$P$表示原始（primal）问题。假设某个$x$不满足原始的约束条件\ref{C2}或者\ref{C3}，也即存在某个$i$，使得$c_i(x) >0$ 或者$h_i(x) \ne 0$，那么总能找到一个$\alpha_i$或者$\beta_i$，使得
$$\theta_P(x)=\mathop{\max}\limits_{\alpha,\beta;\alpha_i\ge 0}L(x,\alpha, \beta)=\infty$$
相反的，如果约束条件满足，那么$\theta_P(x)=f(x)$，也即
\[
\theta_P(x)=\left\{
\begin{array}{ll}
f(x) & \text{$x$满足约束条件} \\
\infty & \text{否则}
\end{array}
\right.
\]
由此可以看出，$\theta_P(x)$在原始问题约束条件满足时与优化目标$f(x)$有相同的值，而当约束条件不满足时，$\theta_P(x)$的值是$\infty$，所以有下列等式
\[
\mathop{\min}\limits_{x}\theta_P(x)=\mathop{\min}\limits_{x} \mathop{\max}\limits_{\alpha,\beta;\alpha_i\ge 0}L(x,\alpha, \beta)
\]
定义原始问题的最优解
$$p^{\star} = \mathop{\min}\limits_{x}\theta_P(x)$$

\subsection{拉格朗日对偶问题}
首先定义
\[
\theta_D(\alpha, \beta)=\mathop{\min}\limits_{x}L(x,\alpha, \beta)
\]
$D$表示对偶（dual）问题。$\theta_D$其实是$L(x,\alpha, \beta)$关于$x$的最小值，而这个最小值是以$\alpha,\beta$为参数的。对这个最小值进行极大化，得到对偶问题
$$\mathop{\max}\limits_{\alpha, \beta;\alpha_i\ge 0}\theta_D(\alpha, \beta)=\mathop{\max}\limits_{\alpha, \beta;\alpha_i\ge 0}\mathop{\min}\limits_{x}L(x,\alpha, \beta)$$
对偶问题跟原始问题在形式上一样，除了max和min的顺序不同。

定义对偶问题的解
$$d^\star = \mathop{\max}\limits_{\alpha, \beta;\alpha_i\ge 0}\theta_D(\alpha, \beta)$$

\subsection{原始问题与对偶问题的关系及KKT条件}
容易证明，有
$$d^\star = \mathop{\max}\limits_{\alpha, \beta;\alpha_i\ge 0}\mathop{\min}\limits_{x}L(x,\alpha, \beta)
\le \mathop{\min}\limits_{x} \mathop{\max}\limits_{\alpha,\beta;\alpha_i\ge 0}L(x,\alpha, \beta)=p^{\star}$$
事实上，函数的max min永远不大于函数的min max。

下面描述原始问题与对偶问题解相同的充要条件，也称为Karush-Kuhn-Tucker（KKT）条件。设$x^\ast$是原始问题的解，$\alpha^\ast,\beta^\ast$是对偶问题的解，假设$f(x),c_i(x)$是凸函数，$h_j(x)$是仿射函数，并且不等式约束$c_i(x)$严格可行，即$\exists x(\forall i (c_i(x)<0))$，则原始问题与对偶问题解相同（即$p^\star=d^\star=L(x^\ast,\alpha^\ast,\beta^\ast)$）的充要条件是
\begin{align}
\nabla_xL(x^\ast,\alpha^\ast,\beta^\ast) &=0 \tag{KKT1}\\
\nabla_\alpha L(x^\ast,\alpha^\ast,\beta^\ast) &=0 \tag{KKT2}\\
\nabla_\beta L(x^\ast,\alpha^\ast,\beta^\ast) &=0 \tag{KKT3}\\
\alpha_i^\ast c_i(x^\ast) = 0,i&=1,2,\cdots, k \tag{KKT4}\\
c_i(x^\ast) \le 0,i&=1,2,\cdots, k \tag{KKT5}\\
\alpha_i^\ast \ge 0, i&=1,2,\cdots, k \tag{KKT6}\\
h_j(x^\ast) = 0, j &=1,2,\cdots,l \tag{KKT7}
\end{align}
条件KKT5称为对偶互补条件（dual complementarity condition）。由此条件可知，当$\alpha_i^\ast > 0$时，$c_i(x^\ast)=0$。在SVM 模型中，此时向量$x^\ast$与分离超平面的函数间隔为1，这种向量称之为支持向量（support vector），因为只有这种向量对模型的训练是有价值的。
\section{梯度下降法（gradient descent）}
梯度下降法或者最速下降法（steepest descent）是求解无约束最优化问题的方法。特点是实现起来比较简单。其原理是如果函数$f(x)$在点$a$处可微且有定义，那么函数$f(x)$在$a$点沿着梯度的反方向，即$-\nabla f(a)$，下降最快。

所以，可以从一个初始值$x_0$出发，沿梯度反方向迭代的更新解。如下
$$x_{n+1} = x_n - \alpha\nabla f(x_n)$$
直到$x_n$的值不再发生变化，或者变化很小，此时，$x_n$等于或者接近$f(x)$的极小值。$\alpha$称为学习率（learning rate）。$\alpha$值过大，可能会在最小值附近振荡。$\alpha$值过小，可能学习的时间比较长。同时，$\alpha$值的选取可以是预先设定的固定值，也可以是根据解更新的情况变化的值。

梯度下降法的一个问题在于，能否得到最优解取决于初始值的选取。
\section{牛顿法和拟牛顿法}
\subsection{牛顿法（Newton Method）}
牛顿法，或牛顿-拉夫逊法（Newton-Raphson Method）也是求解无约束优化问题的常用方法。牛顿法是二阶收敛的算法（不仅考虑梯度方向，同时考虑梯度的梯度），而梯度下降法是一阶收敛的，因此牛顿法的收敛速度比梯度下降快。换句话说，牛顿法用二次曲面来拟合当前所在位置的局部曲面，然后按照曲率最大的方向下降。而梯度法是用一个平面去拟合局部曲面，然后按照平面的法向量的方向下降。但是一次迭代的代价比较高，因为需要计算矩阵的逆。

考虑无约束的最优化问题
$$\mathop{\min}\limits_{x\in \mathbb{R}^n} f(x)$$
假设$f(x)$有二阶连续偏导数，且设第$k$次迭代的解为$x_k$，将$f(x)$在点$x_k$处进行二阶泰勒展开
$$f(x)=f(x_k)+(x-x_k)f^{\prime}(x_k)+\frac{1}{2}(x-x_k)^2f^{\prime\prime}(x_k)$$
函数$f(x)$在下次迭代点$x_{k+1}$处取得极值的必要条件是$f^{\prime}(x_{k+1})=0$，即
$$f^{\prime}(x)|_{x_{k+1}}=f^{\prime}(x_k)+(x_{k+1}-x_{k})f^{\prime\prime}(x_{k})=0$$
解上式得到
$$x_{k+1}=x_{k}-\frac{f^{\prime}(x_{k})}{f^{\prime\prime}(x_{k})}$$
迭代停止的条件可以设定为$f^{\prime}(x_{k})<\epsilon$。

当$x$是向量的时候，其一阶导数要修改成梯度的形式，二阶导数修改成其Hessian矩阵，即
$$f(x)=f(x_k)+(x-x_k)\nabla_{x}f(x_k)+\frac{1}{2}(x-x_k)^TH(x_k)(x-x_{k})$$
当$H(x_{k})$是正定矩阵时，$f(x)$的极值为极小值。其更新公式是
$$x_{k+1}=x_{k}-H^{-1}(x_{k})\nabla_{x}f(x_{k})$$
迭代终止的条件$\nabla_xf(x_{k})<\epsilon$.

当初始点离极值点较远时，牛顿法可能不收敛，因为此时由Hessian矩阵的逆矩阵和梯度规定的牛顿方向不一定是下降方向。
\subsection{拟牛顿法（Quasi Newton Method）}
牛顿法中Hessian矩阵求逆比较复杂，所以实际中会考虑用一个近似的正定对称矩阵来代替Hessian矩阵，这就是拟牛顿法。不同的替代方法形成了不同的拟牛顿法。

\subsubsection{拟牛顿条件}
首先将函数$f(x)$在第$k$次迭代点处泰勒展开
$$f(x)=f(x_k)+(x-x_k)\nabla_{x}f(x_k)+\frac{1}{2}(x-x_k)^TH(x_k)(x-x_{k})$$
求其此时的梯度
$$\nabla f(x) = \nabla f(x_{k}) + (x-x_{k})H(x_{k})$$
令$x=x_{k+1}$，
$$\nabla f(x_{k+1}) - \nabla f(x_{k}) = (x_{k+1}-x_{k})H(x_{k})$$
左边是梯度的变化，右边$(x_{k+1}-x{k})$是自变量的变化，分别记为
\begin{align}
g_{k} &= \nabla f(x_{k+1}) - \nabla f(x_{k}) \notag \\
\delta_{k} &= x_{k+1}-x_{k} \notag
\end{align}
得到拟牛顿条件
\begin{equation}
g_{k} = H_{k}\delta_{k}
\end{equation}
或
\begin{equation}
H_{k}^{-1}g_{k} = \delta_{k}
\end{equation}

拟牛顿法选择$G_{k} \approx H_{k}^{-1}$，或者$B_{k} \approx H_{k}$。而且，尽量使得近似矩阵的更新方式为迭代更新
$$G_{k+1}=G_{k}+\Delta G_{k}$$
且$G_{k}$满足拟牛顿条件
\begin{equation}
G_{k+1}g_{k}=\delta_{k}
\end{equation}
一般令$G_0 = I$为单位阵，所以只需要找到$\Delta G_{k}$即可。
\subsubsection{DFP算法}
DFP算法最早由William C. Davidon于1959年提出，后由Roger Fletcher和Michael J.D. Powell发展和完善。DFP算法令校正矩阵为
$$\Delta G_{k} = \frac{\delta_{k}\delta_{k}^{T}}{\delta_{k}^{T}g_{k}}-\frac{G_{k}g_{k}g_{k}^{T}G_{k}}{g_{k}^{T}G_{k}g_{k}}$$
构造过程主要的规则在于保证如果初始矩阵$G_{0}$正定，那么每个$G_{k}$都是正定。

DFP算法

输入：目标函数$f(x)$，梯度$\nabla f(x)$，精度要求$\epsilon$

输出：$f(x)$的极小值点$x^{\star}$ \\
（1）选定初始点$x_0$，取$G_0$为正定对称矩阵（单位阵），置$k=0$ \\
（2）计算$\nabla f(x_k)$，若$\| \nabla f(x_k) \|< \epsilon$，令$x^{\star}=x_k$，停止计算\\
（3）令$p_k=-G_k\nabla f(x_k)$\\
（4）一维搜索：求$\lambda_k$使得
$$f(x_k+\lambda_k p_k)=\mathop{\min}\limits_{\lambda \ge 0}f(x_k+\lambda p_k)$$
（5）令$x_{k+1}=x_{k}+\lambda_k p_k$\\
（6）计算$\nabla f(x_{k+1})$，若$\| \nabla f(x_{k+1}) \|< \epsilon$，令$x^{\star}=x_{k+1}$，停止计算；否则，按$G_{k+1}=G_{k}+\Delta G_{k}$计算$G_{k+1}$\\
（7）置$k=k+1$，转（3）

\subsubsection{BFGS算法}
BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）是最流行的拟牛顿法。DFP用一个正定矩阵来近似$H^{-1}$，BFGS用一个正定矩阵来近似$H$，其表示如下
\begin{align}
B_{k+1} &= B_k + \Delta B_k\\
\Delta B_k & = \frac{g_kg_k^T}{g_k^T\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}
\end{align}
构造的核心也是满足如果初始矩阵$B_0$是正定的，那么迭代过程中的每一个$B_k$都是正定的。

BFGS算法

输入：目标函数$f(x)$，梯度$\nabla f(x)$，精度$\epsilon$

输出：$f(x)$的极小值点$x^{\star}$\\
（1）选定初始点$x_0$，取$B_0$为正定对称矩阵（单位阵），置$k=0$ \\
（2）计算$\nabla f(x_k)$，若$\| \nabla f(x_k) \|< \epsilon$，令$x^{\star}=x_k$，停止计算\\
（3）由$B_kp_k=-\nabla f(x_k)$求出$p_k$\\
（4）一维搜索：求$\lambda_k$使得
$$f(x_k+\lambda_k p_k)=\mathop{\min}\limits_{\lambda \ge 0}f(x_k+\lambda p_k)$$
（5）令$x_{k+1}=x_{k}+\lambda_k p_k$\\
（6）计算$\nabla f(x_{k+1})$，若$\| \nabla f(x_{k+1}) \|< \epsilon$，令$x^{\star}=x_{k+1}$，停止计算；否则，按$B_{k+1}=B_{k}+\Delta B_{k}$计算$B_{k+1}$\\
（7）置$k=k+1$，转（3）

令$G_k=B_k^{-1}$，对$B_{k+1}=B_k+\Delta B_k$运用两次Sherman-Morrison公式有
$$G_{k+1}=(I-\frac{\delta_k g_k^T}{\delta_k^T g_k})G_k(I-\frac{\delta_k g_k^T}{\delta_k^T g_k})^T+\frac{\delta_k g_k^T}{\delta_k^T g_k}$$
称为BFGS算法关于$G_k$的迭代公式。

Sherman-Morrison公式：假设$A$是$n$阶可逆矩阵，$u$，$v$是$n$维向量，且$A+uv^T$也是可逆矩阵，则
$$(A+uv^T)^{-1}=A^{-1}-\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}$$

记DFP关于$G_k$的迭代公式得到的$G_{k+1}$为$G^{DFP}$，由BFGS得到的记为$G^{BFGS}$，它们都满足拟牛顿公式，且其线性组合
$$G_{k+1}=\alpha G^{DFP} + (1-\alpha)G^{BFGS}$$
也满足拟牛顿条件，而且是正定是。其中$0 \le \alpha \le 1$。这样得到的一类拟牛顿法称为Broyden类算法。

实际在使用的时候使用的是LBFGS算法（Limited-memory-BFGS）。有很成熟的开源实现，直接调用即可。


\section{机器学习概论}

\subsection{生成模型与判别模型}
判别模型（discriminative model）是给定输入$X$，要求预测的输出$Y$的类型。其方法是通过训练数据来学习决策函数$f(x)$或者条件概率分布$p(y|x)$来构造预测的模型。

生成模型（generative model）是通过学习联合分布$p(x,y)$或者$p(x|y)$以及$p(y)$来得到给定$x$的$y$的后验概率$p(y|x)=\frac{p(x,y)}{p(y)}=\frac{p(x|y)p(y)}{p(y)}$。


