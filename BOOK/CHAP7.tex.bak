\chapter{支持向量机}
\thispagestyle{empty}

\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}
%\setlength{\fboxrule}{0pt}\setlength{\fboxsep}{0cm}
%\shadowbox{
%\begin{tcolorbox}[arc=0mm,colback=white,colframe=darkblue,title=学习目标与要求]
%\kai\textcolor{darkblue}{1.~~了解科学计算的一般过程．}\\
%\kai\textcolor{darkblue}{2.~~了解数值计算方法的研究内容和特点．}\\
%\kai\textcolor{darkblue}{3.~~理解数值计算误差的有关概念．}\\
%\kai\textcolor{darkblue}{4.~~掌握数值计算误差的控制方法．}
%\end{tcolorbox}}
%\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}


支持向量机首先是一个二类分类模型。这点与感知机类似。其次，它是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。因为根据感知机训练出来的分离超平面可能不是唯一的，例如一些与分离超平面平行的平面可能也满足分类要求。但是，当考虑到间隔最大化时，这样的平面就可以唯一确定下来。这里需要留意的几个概念包括：特征空间和间隔。

支持向量机的分类。按照训练数据是否线性可分，当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），定义线性可分支持向量机（linear support vector machine in linearly separable case）或者硬间隔支持向量机。当训练数据近似线性可分（也即可能有一些噪声点是会导致原数据不是线性可分的）时，通过软间隔最大化（soft margin maximization），定义线性支持向量机（linear support vector machine），或者软间隔支持向量机。当训练数据线性不可分（也即分离超平面可能是超曲面）时，通过核技巧（kernel trick）和软间隔最大化，定义非线性支持向量机（non-linear support vector machine）。

在支持向量机学习中，拉格朗日对偶法之所以会发生特殊的作用还在于，对偶表示之后数据仅作为Gram矩阵的项出现，而不需要通过单个属性出现。类似地，在决策函数的对偶表示里仅需要与测试点输入的内积。

支持向量机也被有些人认为是现存最好的监督学习算法。

\section{模型}
首先定义支持向量机的输出$y \in \{-1,1\}$。

其次定义支持向量机的参数，将之前一直使用的$\theta$拆分成法向量$w$和截距$b$的形式，支持向量机的训练目的即得到分离超平面（separating hyperplane）$w^Tx+b$。

最后，SVM的判别函数
$$h_{w,b}(x)=g(w^Tx+b)$$
其中
\[g(z)=\left\{
\begin{array}{rcl}
1 & , & z \ge 0 \\
-1 & , & z < 0
\end{array}
\right.
\]
\section{函数间隔和几何间隔（functional and geometric margins）}
训练样本$(x^{(i)}, y^{(i)})$函数间隔定义为
$$\hat\gamma^{(i)}=y^{(i)}(w^Tx^{(i)}+b)$$
不难看出，恒有$\hat\gamma^{(i)}\ge 0$。

所有训练样本与分离超平面之间的函数间隔的最小值定义为
$$\hat\gamma = \mathop{\min}\limits_{i=1,2\cdots,m} \hat\gamma^{(i)}$$

函数间隔可以通过等比例增大$w,b$来增大。比如令$w=2w,b=2b$，则有$\hat\gamma^{(i)}=2\hat\gamma^{(i)}$。所以需要引入几何间隔的概念。

函数间隔实际上并不是点到平面的距离。点到平面的距离称为几何间隔，定义为
$$\gamma^{(i)}=y^{(i)}((\frac{w}{\|w\|})^Tx^{(i)}+\frac{b}{\|w\|})$$
同时，定义几何间隔的最小值
$$\gamma = \mathop{\min}\limits_{i=1,2\cdots,m} \gamma^{(i)}$$
几何间隔不会随着$w,b$的变化而变化，因为只要平面本身没有变，点到平面的距离是不会随着平面的表达方式的变化而变化的。

函数间距与几何间距的关系。
\begin{align}
\gamma^{(i)}&=\frac{\hat\gamma^{(i)}}{\|w\|}\notag \\
\gamma&=\frac{\hat\gamma}{\|w\|}\notag
\end{align}
这个关系对于简化最优化的目标公式有用。

\section{SVM模型}
按照几何间隔最大话，定义SVM的最优化模型
\begin{alignat}{2}
\mathop{\max}\limits_{w,b}\quad & \gamma &{}& \nonumber\\
\mbox{s.t.}\quad
&y^{(i)}((\frac{w}{\|w\|})^Tx^{(i)}+\frac{b}{\|w\|})\ge \gamma, &\quad& i=1,2,\cdots,m \nonumber
\end{alignat}
引入函数间隔与几何间隔的关系，将约束条件变成线性的
\begin{alignat}{2}
\mathop{\max}\limits_{w,b}\quad & \frac{\hat\gamma}{\|w\|} &{}& \nonumber\\
\mbox{s.t.}\quad
&y^{(i)}(w^Tx^{(i)}+b)\ge \hat\gamma, &\quad& i=1,2,\cdots,m \nonumber
\end{alignat}
目标函数$\frac{\hat\gamma}{\|w\|}$不是凸函数（non-convex），目前还没有方法来解决这类优化问题。

从前面关于函数间隔的讨论可以看出，函数间隔本身是可以根据$w,b$的变化（缩放）而变化的。也即，函数间隔对最优化问题没有影响，可以直接令$\hat\gamma=1$。这一点也可以通过缩放$w,b$来实现。另一方面，注意到最大化$\frac{1}{\|w\|}$等价于最小化$\|w\|$（为了便于计算，可以使用$\frac{1}{2}\|w\|^2$），所以可以将上面的优化问题改为
\begin{alignat}{2}
\mathop{\min}\limits_{w,b}\quad & \frac{1}{2}\|w\|^2 &{}& \tag{SVM1} \label{SVM1}\\
\mbox{s.t.}\quad
&y^{(i)}(w^Tx^{(i)}+b)\ge 1, &\quad& i=1,2,\cdots,m \nonumber
\end{alignat}
这样转化为一个凸二次规划问题（convex quadratic programming）。

\section{线性可分SVM}
线性可分支持向量机的模型如\ref{SVM1}所示。求解的方法是首先构造\ref{SVM1}的拉格朗日函数。
\begin{equation}
L(w,b,\alpha)=\frac{1}{2}\|w\|^2-\sum_{i=1}^{m}\alpha_i [y^{(i)}(w^Tx^{(i)}+b)-1] \label{LagSVM1}
\end{equation}
注意，先将\ref{SVM1}中的约束条件改成$\le 0$的形式，然后应用拉格朗日乘子法。$w,b$是目标函数的参数，$\alpha$是拉格朗日乘子，$m$是训练样本的个数。

（1）先求$\theta_D(\alpha)=\mathop{\min}\limits_{w,b} L(w,b,\alpha)$。令
\begin{align}
\nabla_w L(w,b,\alpha) &= w-\sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}=0 \notag\\
\nabla_b L(w,b,\alpha) &=\sum_{i=1}^m \alpha_i y^{(i)}=0 \notag 
\end{align}
解得
\begin{align}
& w = \sum_{i=1}^m \alpha_i y^{(i)}x^{(i)} \notag \\
& \sum_{i=1}^m \alpha_i y^{(i)} =0 \notag
\end{align}
将上式代入\ref{LagSVM1}中，得到
\begin{align}
L(w,b,\alpha) &=\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)}x^{(i)}x^{(j)}-\sum_{i=1}^{m}\alpha_i [y^{(i)}((\sum_{i=j}^m \alpha_j y^{(j)}x^{(j)})x^{(i)}+b)]+\sum_{i=1}^m \alpha_i \notag\\
&= -\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)}x^{(i)}x^{(j)}+\sum_{i=1}^m \alpha_i \notag
\end{align}

（2）求对偶问题关于$\alpha$的最优解
\begin{alignat}{2}
\mathop{\max}\limits_{\alpha}\quad & W(\alpha)=-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)}\langle x^{(i)},x^{(j)} \rangle +\sum_{i=1}^m \alpha_i &{}& \notag\\
\mbox{s.t.}\quad
&\alpha_i\ge 0, i=1,2,\cdots,m  \nonumber \\
&\sum_{i=1}^m \alpha_i y^{(i)} =0 \nonumber
\end{alignat}

设$\alpha^\ast$是上述对偶问题的解，则原始问题的解为
\begin{align}
w^\ast &= \sum_{i=1}^m \alpha_i^\ast y^{(i)}x^{(i)} \notag \\
b^\ast &= y^{(j)}-\sum_{i=1}^m \alpha_i^\ast y^{(i)}\langle x^{(i)}, x^{(j)}\rangle \notag
\end{align}
其中$(x^{(j)}, y^{(j)})$是支持向量。$b^\ast$可以通过$y^{(j)}((w^\ast)^Tx^{(j)} + b^\ast)-1=0$求得。或者如Andrew Ng的讲义中所说，用支持向量的正例与负例所对应的与分离超平面平行的平面的截距的平均值来定义$b^\ast$
$$b^\ast = -\frac{\mathop{\max}\limits_{i:y^{(i)}=-1} (w^\ast)^Tx^{(i)} + \mathop{\min}\limits_{i:y^{(i)}=1} (w^\ast)^Tx^{(i)}}{2}$$

根据上述解，可以求得分离超平面
\begin{align}
w^Tx+b &= (\sum_{i=1}^m \alpha_i^\ast y^{(i)}x^{(i)})^Tx+b \notag\\
&= \sum_{i=1}^m \alpha_i^\ast y^{(i)}\langle x^{(i)}, x \rangle +b \notag\\
&=0 \notag
\end{align}
分类决策函数
\[
f(x)=sign(\sum_{i=1}^m \alpha_i^\ast y^{(i)}\langle x^{(i)}, x \rangle +b)
\]

注意，只有支持向量的$\alpha>0$，其他向量的$\alpha=0$。也就是说，对于一个新的待分类的点$x$，分类的结果只取决于$x$与支持向量的内积。而支持向量的个数往往比训练集中全部向量的个数小得多，这一点对于支持向量机很重要。
\section{线性SVM}
当训练样本中存在一些特异点（outlier）时，训练样本可能不是线性可分的。假设去掉特异点之后的训练样本是线性可分的，则可以构造线性SVM。线性不可分意味着某些样本点$(x^{(j)}, y^{(j)})$不满足函数间隔大于等于1的约束条件。为了解决这个问题，可以通过对每个训练样本引入松弛变量$\xi_j\ge 0$，使得函数间隔加上松弛变量大于等于1.这样，约束条件变成
$$y^{(j)}(w^Tx^{(j)}+b)+\xi_j \ge 1$$
同时，对每个松弛变量$\xi_j$，支付一个代价$\xi_j$，则目标函数变成
$$\frac{1}{2}\|w\|^2+C\sum_{i=1}^m \xi_i$$
其中$C>0$称为惩罚参数。$C$值大时，对误分类的惩罚增大，$C$值小时，对误分类的惩罚减小。线性SVM的优化形式可以表述为
\begin{alignat}{2}
\mathop{\min}\limits_{w,b,\xi}\quad & \frac{1}{2}\|w\|^2+C\sum_{i=1}^m \xi_i &{}& \tag{SVM2} \label{SVM2}\\
\mbox{s.t.}\quad
&y^{(i)}(w^Tx^{(i)}+b)\ge 1-\xi_i , &\quad& i=1,2,\cdots,m \nonumber\\
&\xi_i \ge 0, &\quad& i = 1,2,\cdots, m \nonumber
\end{alignat}
使用拉格朗日乘子法得到其对偶形式
\begin{alignat}{2}
\mathop{\max}\limits_{\alpha}\quad & W(\alpha)=-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)}\langle x^{(i)},x^{(j)} \rangle +\sum_{i=1}^m \alpha_i &{}& \notag\\
\mbox{s.t.}\quad
&0 \le \alpha_i \le C, i=1,2,\cdots,m  \nonumber \\
&\sum_{i=1}^m \alpha_i y^{(i)} =0 \nonumber
\end{alignat}
有意思的是，\ref{SVM2}的对偶形式跟\ref{SVM1}的对偶形式的差别仅仅是$0\le \alpha_i$变成了$0\le \alpha_i \le C$。同时，KKT对偶互补性条件变成
\begin{align}
\alpha_i =0 &\Rightarrow y^{(i)}(w^Tx^{(i)}+b)\ge 1 \\
\alpha_i =C &\Rightarrow y^{(i)}(w^Tx^{(i)}+b)\le 1 \\
0 <\alpha_i <C &\Rightarrow y^{(i)}(w^Tx^{(i)}+b)= 1
\end{align}

\section{非线性SVM}
\subsection{非线性分类问题}
\begin{center}
\includegraphics[width=3.0in,height=1.75in]{book/pig2.jpg}
\end{center}

上图左边显示了一个非线性分类问题，使用一个分离超平面无法将正负实例点完全分开，但是使用一个分离超曲面（椭圆）$w_1x_1^2+w_2x_2^2+b=0$ 可以分开。我们做一些变化，令
\begin{align}
z &= (z_1, z_2)^T\notag \\
&=\phi(x)\notag\\
&=(x_1^2,x_2^2)^T \notag
\end{align}
则左图的分离超曲面可以表示为$w_1z_1+w_2z_2+b=0$。如右图所示，这样通过将原输入空间$\mathcal{X}\subset \mathbb{R}^2$映射到特征空间$\mathcal{Z}\subset \mathbb{R}^2$，使得原来线性不可分的数据变成了线性可分的。由此可见，解决线性不可分问题的关键在于选择映射函数$\phi$.

假设原始训练数据集为
$$T=\{(x^{(i)}, y^{(i)})|i=1,2,\cdots,m\}\in (\mathbb{R}^n \times \mathcal{Y})^m$$
其中，$x^{(i)}\in \mathbb{R}^n$，$y^{(i)}\in \mathcal{Y}=\{-1,1\}$。引入映射$\phi:\mathbb{R}^n \rightarrow \mathcal{H}$，使得$z=\phi(x)\in \mathcal{H}$。则原始训练集变成
$$T_\phi=\{(z^{(i)}, y^{(i)})|i=1,2,\cdots,m\}\in (\mathcal{H} \times \mathcal{Y})^m$$
其中，$z^{(i)}=\phi(x^{(i)})\in \mathcal{H}$，$y^{(i)}\in \mathcal{Y}=\{-1,1\}$。

原问题变为在希尔伯特空间$\mathcal{H}$中求得线性划分超平面$(w^\ast)^Tz+b^\ast=0$，从而导出$\mathbb{R}^n$空间中的分离超曲面$(w^\ast)^T\phi(x)+b^\ast=0$。

在希尔伯特空间$\mathcal{H}$中构造对应分类问题的最优化形式，并应用拉格朗日乘子法（与欧式空间中的拉格朗日乘子法相同，同样需要满足KKT条件），不难得出希尔伯特空间中分类问题的如下对偶形式
\begin{alignat}{2}
\mathop{\max}\limits_{\alpha}\quad & W(\alpha)=-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)}\langle \phi(x^{(i)}),\phi(x^{(j)}) \rangle +\sum_{i=1}^m \alpha_i &{}& \notag\\
\mbox{s.t.}\quad
&0 \le \alpha_i \le C, i=1,2,\cdots,m  \nonumber \\
&\sum_{i=1}^m \alpha_i y^{(i)} =0 \nonumber
\end{alignat}
并且有
\begin{align}
w^\ast &= \sum_{i=1}^m \alpha_i^\ast y^{(i)}\phi(x^{(i)}) \notag \\
b^\ast &= y^{(j)}-\sum_{i=1}^m \alpha_i^\ast y^{(i)}\langle \phi(x^{(i)}), \phi(x^{(j)})\rangle \notag
\end{align}
分类决策函数
$$f(x)=sign(\sum_{i=1}^m \alpha_i^\ast y^{(i)}\langle \phi(x^{(i)}), \phi(x) \rangle +b)$$

\subsection{核（Kernel）}
在希尔伯特空间中，变化$\phi$完全是通过内积的形式$\langle \phi(x^{(i)}),\phi(x^{(j)}) \rangle$和$\langle \phi(x^{(i)}),\phi(x) \rangle$出现，因此可以考虑使用一个函数代替$\phi$的内积形式$\langle \phi(\bullet),\phi(\bullet) \rangle$，这个函数称为核函数，定义如下
\[K(x,x^\prime)=\langle \phi(x),\phi(x^\prime) \rangle\]

内积实际上描述的是向量之间的相似度（因为内积可以同时表征向量之间的长度差、向量之间的距离和向量之间的夹角）。在将原空间映射为希尔伯特空间时，向量之间的相似度同时映射到希尔伯特空间。但是映射后的希尔伯特空间可能维数非常高（可能是无穷维，比如采用高斯核函数时，映射函数映射后的希尔伯特空间是无穷维的），而采用核技巧，又将希尔伯特空间中的内积运算转化为原空间中的代数运算。这样有效的降低了运算的复杂度。注意，核函数$K(x,x^\prime)$要保持内积所表征的向量之间的相似度，即当$x$与$x^\ast$相似时（比如平行），$K(x,x^\prime)$会很大。但是当$x$和$x^\ast$不相似时（比如正交），$K(x,x^\prime)$会很小。

为了描述核函数的必要特性，首先定义核矩阵（或者Gram矩阵）。
\definibox{核矩阵}{给定函数$K(x,x^\prime):\mathbb{R}^n\times \mathbb{R}^n\rightarrow \mathbb{R}$，对任意的$x^{(i)}\in \mathbb{R}^n$，$i=1,2,\cdots,m$，函数$K(x,x^\prime)$对应的核矩阵为
$$K=[K(x^{(i)},x^{(j)})]_{m\times m}$$}
核矩阵的对称性显而易见。
\theorebox{核函数的特征}{定义在$\mathbb{R}^n\times \mathbb{R}^n$上的对称函数$K(x,x^\prime)$是核函数的充要条件是对任意的$x^{(i)}\in \mathbb{R}^n$，其核矩阵$K$半正定。}

应该注意，核技巧是一种比SVM更为广泛的技巧。

\subsection{常用核函数}
1.多项式核函数（polynomial kernel function）

$d$阶其次多项式函数
$$K(x,z)=(x\cdot z)^p$$
和$d$阶非齐次多项式函数
$$K(x,z)=(x\cdot z+1)^p$$
都是核函数。

2.高斯核函数（Gaussian kernel function）
$$K(x,z)=exp(-\frac{\|x-z\|^2}{2\sigma^2})$$
