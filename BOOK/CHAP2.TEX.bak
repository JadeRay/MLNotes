\chapter{回归分析}
\thispagestyle{empty}

\setlength{\fboxrule}{1pt}\setlength{\fboxsep}{4pt}


%\section{模型}
给定数据集
    $$T=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})\},$$
    其中，$x^{(i)}=(x_0, x_1, \cdots, x_n) \in \mathcal{X} = \mathbb{R}^{n+1}$，$y^{(i)} \in \mathcal{Y}$, $i=0,2,\cdots, m$，且$x_0=1$ （表示截距，intercept term）。回归分析的任务是找出输入$x$ 与输出$y$之间的关系。

\section{线性回归}
假设输入与输出之间满足的关系是线性的，$theta$称为参数（parameters）或者权重（weights）
    $$y=h_{\theta}(x)={\theta}^Tx=\sum_{i=0}^{n}\theta_ix_i, \qquad \theta \in \mathbb{R}^{n+1}$$
    
对于这个模型，需要有一个损失函数（cost function）来表示其对训练数据的拟合程度
    $$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2.$$
这个被称为最小二乘方效用函数（least-square cost function）。

则这个问题的求解可以表述为如下的无约束最优化问题
$$\mathop{\min}\limits_\theta J(\theta)$$

\subsection{直接求解}
直接求$J(\theta)$对$\theta$的极值。首先定义设计矩阵（design matrix）$X$
\begin{align}
X&=\begin{bmatrix}(x^{(1)})^T & (x^{(2)})^T & \cdots & (x^{(m)})^T \end{bmatrix}^T  \notag \\
Y&=\begin{bmatrix}y^{(1)} & y^{(2)} & \cdots & y^{(m)} \end{bmatrix}^T  \notag
\end{align}    
则有
$$
X\theta - Y = \begin{bmatrix}h_{\theta}(x^{(1)})-y^{(1)} & \cdots & h_{\theta}(x^{(m)})-y^{(m)}\end{bmatrix}^T
$$
考虑到$z^Tz=\sum_iz_i^2$，有
$$
J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

使用公式$\nabla_{A}trABA^TC = CAB+C^TAB^T$，且由于$J(\theta)$只是个实数，所以
\begin{align}
\nabla_{\theta}J(\theta)&=\nabla_{\theta}trJ(\theta) \notag\\
&=X^TX\theta-X^TY \notag
\end{align}
令$\nabla_{\theta}J(\theta)=0$得到
$$\theta=(X^TX)^{-1}X^TY$$

不过，这个公式用来直接计算$\theta$不现实，因为矩阵求逆比较麻烦，同时可能会是数值不稳定的矩阵。
\subsection{批处理梯度下降法（batch gradient descent）}
$J(\theta)$对$\theta$的梯度
$$\frac{\partial J(\theta)}{\partial \theta}=\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}$$
所以，更新公式为
\begin{align}
\theta_{n+1} &= \theta_n - \alpha\frac{\partial J(\theta)}{\partial \theta} \notag \\
&=\theta_n - \alpha \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)} \notag
\end{align}
\subsection{随机梯度下降法（stochastic gradient descent）}


\section{逻辑斯蒂回归}


