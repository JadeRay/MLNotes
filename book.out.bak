\BOOKMARK [0][-]{chapter.1}{ 第1章 绪论}{}% 1
\BOOKMARK [1][-]{section.1.1}{1.1 凸函数}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{1.2 最优化问题}{chapter.1}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{1.2.1 原始问题}{section.1.2}% 4
\BOOKMARK [2][-]{subsection.1.2.2}{1.2.2 拉格朗日对偶问题}{section.1.2}% 5
\BOOKMARK [2][-]{subsection.1.2.3}{1.2.3 原始问题与对偶问题的关系及KKT条件}{section.1.2}% 6
\BOOKMARK [1][-]{section.1.3}{1.3 梯度下降法（gradient descent）}{chapter.1}% 7
\BOOKMARK [1][-]{section.1.4}{1.4 牛顿法和拟牛顿法}{chapter.1}% 8
\BOOKMARK [2][-]{subsection.1.4.1}{1.4.1 牛顿法（Newton Method）}{section.1.4}% 9
\BOOKMARK [2][-]{subsection.1.4.2}{1.4.2 拟牛顿法（Quasi Newton Method）}{section.1.4}% 10
\BOOKMARK [3][-]{subsubsection.1.4.2.1}{1.4.2.1 拟牛顿条件}{subsection.1.4.2}% 11
\BOOKMARK [3][-]{subsubsection.1.4.2.2}{1.4.2.2 DFP算法}{subsection.1.4.2}% 12
\BOOKMARK [3][-]{subsubsection.1.4.2.3}{1.4.2.3 BFGS算法}{subsection.1.4.2}% 13
\BOOKMARK [1][-]{section.1.5}{1.5 机器学习概论}{chapter.1}% 14
\BOOKMARK [2][-]{subsection.1.5.1}{1.5.1 生成模型与判别模型}{section.1.5}% 15
\BOOKMARK [0][-]{chapter.2}{ 第2章 回归}{}% 16
\BOOKMARK [1][-]{section.2.1}{2.1 线性回归}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.1.1}{2.1.1 直接求解}{section.2.1}% 18
\BOOKMARK [2][-]{subsection.2.1.2}{2.1.2 牛顿法（Newton's Method）}{section.2.1}% 19
\BOOKMARK [2][-]{subsection.2.1.3}{2.1.3 批处理梯度下降法（batch gradient descent）}{section.2.1}% 20
\BOOKMARK [2][-]{subsection.2.1.4}{2.1.4 随机梯度下降法（stochastic gradient descent）}{section.2.1}% 21
\BOOKMARK [1][-]{section.2.2}{2.2 局部加权线性回归（LWR）}{chapter.2}% 22
\BOOKMARK [1][-]{section.2.3}{2.3 Logistic回归}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.3.1}{2.3.1 二项逻辑斯谛回归}{section.2.3}% 24
\BOOKMARK [2][-]{subsection.2.3.2}{2.3.2 多项逻辑斯谛回归}{section.2.3}% 25
\BOOKMARK [1][-]{section.2.4}{2.4 广义线性模型}{chapter.2}% 26
\BOOKMARK [2][-]{subsection.2.4.1}{2.4.1 指数分布族（the exponential family）}{section.2.4}% 27
\BOOKMARK [2][-]{subsection.2.4.2}{2.4.2 构造GLMs}{section.2.4}% 28
\BOOKMARK [3][-]{subsubsection.2.4.2.1}{2.4.2.1 最小二乘法（Ordinary Least Square）}{subsection.2.4.2}% 29
\BOOKMARK [3][-]{subsubsection.2.4.2.2}{2.4.2.2 Logistic Regression}{subsection.2.4.2}% 30
\BOOKMARK [3][-]{subsubsection.2.4.2.3}{2.4.2.3 Softmax Regression}{subsection.2.4.2}% 31
\BOOKMARK [2][-]{subsection.2.4.3}{2.4.3 总结}{section.2.4}% 32
\BOOKMARK [0][-]{chapter.3}{ 第3章 生成算法}{}% 33
\BOOKMARK [1][-]{section.3.1}{3.1 高斯判别分析（Gaussian Discriminant Analysis）}{chapter.3}% 34
\BOOKMARK [2][-]{subsection.3.1.1}{3.1.1 多元正态分布（Multivariate Normal Distribution）}{section.3.1}% 35
\BOOKMARK [2][-]{subsection.3.1.2}{3.1.2 高斯判别分析（GDA）}{section.3.1}% 36
\BOOKMARK [2][-]{subsection.3.1.3}{3.1.3 高斯判别分析与logistic回归的对比}{section.3.1}% 37
\BOOKMARK [1][-]{section.3.2}{3.2 朴素贝叶斯法}{chapter.3}% 38
\BOOKMARK [2][-]{subsection.3.2.1}{3.2.1 贝叶斯公式}{section.3.2}% 39
\BOOKMARK [2][-]{subsection.3.2.2}{3.2.2 朴素贝叶斯法}{section.3.2}% 40
\BOOKMARK [3][-]{subsubsection.3.2.2.1}{3.2.2.1 模型}{subsection.3.2.2}% 41
\BOOKMARK [3][-]{subsubsection.3.2.2.2}{3.2.2.2 实现}{subsection.3.2.2}% 42
\BOOKMARK [0][-]{chapter.4}{ 第4章 神经网络}{}% 43
\BOOKMARK [0][-]{chapter.5}{ 第5章 决策树}{}% 44
\BOOKMARK [0][-]{chapter.6}{ 第6章 最大熵}{}% 45
\BOOKMARK [0][-]{chapter.7}{ 第7章 支持向量机}{}% 46
\BOOKMARK [1][-]{section.7.1}{7.1 模型}{chapter.7}% 47
\BOOKMARK [1][-]{section.7.2}{7.2 函数间隔和几何间隔（functional and geometric margins）}{chapter.7}% 48
\BOOKMARK [1][-]{section.7.3}{7.3 SVM模型}{chapter.7}% 49
\BOOKMARK [1][-]{section.7.4}{7.4 线性可分SVM}{chapter.7}% 50
\BOOKMARK [1][-]{section.7.5}{7.5 线性SVM}{chapter.7}% 51
\BOOKMARK [1][-]{section.7.6}{7.6 非线性SVM}{chapter.7}% 52
\BOOKMARK [2][-]{subsection.7.6.1}{7.6.1 非线性分类问题}{section.7.6}% 53
\BOOKMARK [2][-]{subsection.7.6.2}{7.6.2 核（Kernel）}{section.7.6}% 54
\BOOKMARK [2][-]{subsection.7.6.3}{7.6.3 常用核函数}{section.7.6}% 55
\BOOKMARK [1][-]{section.7.7}{7.7 SMO算法}{chapter.7}% 56
\BOOKMARK [2][-]{subsection.7.7.1}{7.7.1 坐标上升法（Coordinate Ascent）}{section.7.7}% 57
\BOOKMARK [2][-]{subsection.7.7.2}{7.7.2 SMO}{section.7.7}% 58
\BOOKMARK [1][-]{section.7.8}{7.8 SVM的软件包}{chapter.7}% 59
\BOOKMARK [0][-]{chapter.8}{ 第8章 提升方法}{}% 60
\BOOKMARK [1][-]{section.8.1}{8.1 理论背景}{chapter.8}% 61
\BOOKMARK [0][-]{chapter.9}{ 第9章 EM方法}{}% 62
\BOOKMARK [1][-]{section.9.1}{9.1 Jensen不等式}{chapter.9}% 63
\BOOKMARK [0][-]{chapter.10}{ 第10章 隐马尔可夫模型}{}% 64
\BOOKMARK [1][-]{section.10.1}{10.1 模型}{chapter.10}% 65
\BOOKMARK [1][-]{section.10.2}{10.2 概率计算问题}{chapter.10}% 66
\BOOKMARK [2][-]{subsection.10.2.1}{10.2.1 直接计算}{section.10.2}% 67
\BOOKMARK [2][-]{subsection.10.2.2}{10.2.2 前向算法（forward algorithm）}{section.10.2}% 68
\BOOKMARK [2][-]{subsection.10.2.3}{10.2.3 后向算法（backward algorithm）}{section.10.2}% 69
\BOOKMARK [1][-]{section.10.3}{10.3 学习问题}{chapter.10}% 70
\BOOKMARK [1][-]{section.10.4}{10.4 预测问题}{chapter.10}% 71
\BOOKMARK [0][-]{chapter.11}{ 第11章 附录}{}% 72
\BOOKMARK [1][-]{section.11.1}{11.1 线性代数}{chapter.11}% 73
\BOOKMARK [2][-]{subsection.11.1.1}{11.1.1 空间}{section.11.1}% 74
\BOOKMARK [1][-]{section.11.2}{11.2 矩阵分析（matrix analysis）}{chapter.11}% 75
\BOOKMARK [2][-]{subsection.11.2.1}{11.2.1 迹\(Trace\)和导数\(matrix derivatives\)}{section.11.2}% 76
\BOOKMARK [1][-]{section.11.3}{11.3 常用不等式}{chapter.11}% 77
\BOOKMARK [2][-]{subsection.11.3.1}{11.3.1 柯西不等式（Cauchy Inequality）}{section.11.3}% 78
\BOOKMARK [2][-]{subsection.11.3.2}{11.3.2 赫尔德不等式（Hlder Inequality）}{section.11.3}% 79
\BOOKMARK [2][-]{subsection.11.3.3}{11.3.3 闵可夫斯基不等式（Minkowski Inequality）}{section.11.3}% 80
\BOOKMARK [0][-]{chapter*.2}{ 参考文献}{}% 81
