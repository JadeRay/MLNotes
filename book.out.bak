\BOOKMARK [0][-]{chapter.1}{ 第1章 绪论}{}% 1
\BOOKMARK [1][-]{section.1.1}{1.1 凸函数}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{1.2 最优化问题}{chapter.1}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{1.2.1 原始问题}{section.1.2}% 4
\BOOKMARK [2][-]{subsection.1.2.2}{1.2.2 拉格朗日对偶问题}{section.1.2}% 5
\BOOKMARK [2][-]{subsection.1.2.3}{1.2.3 原始问题与对偶问题的关系及KKT条件}{section.1.2}% 6
\BOOKMARK [1][-]{section.1.3}{1.3 梯度下降法（gradient descent）}{chapter.1}% 7
\BOOKMARK [1][-]{section.1.4}{1.4 牛顿法和拟牛顿法}{chapter.1}% 8
\BOOKMARK [2][-]{subsection.1.4.1}{1.4.1 牛顿法（Newton Method）}{section.1.4}% 9
\BOOKMARK [2][-]{subsection.1.4.2}{1.4.2 拟牛顿法（Quasi Newton Method）}{section.1.4}% 10
\BOOKMARK [3][-]{subsubsection.1.4.2.1}{1.4.2.1 拟牛顿条件}{subsection.1.4.2}% 11
\BOOKMARK [3][-]{subsubsection.1.4.2.2}{1.4.2.2 DFP算法}{subsection.1.4.2}% 12
\BOOKMARK [3][-]{subsubsection.1.4.2.3}{1.4.2.3 BFGS算法}{subsection.1.4.2}% 13
\BOOKMARK [1][-]{section.1.5}{1.5 机器学习概论}{chapter.1}% 14
\BOOKMARK [2][-]{subsection.1.5.1}{1.5.1 生成模型与判别模型}{section.1.5}% 15
\BOOKMARK [0][-]{chapter.2}{ 第2章 回归}{}% 16
\BOOKMARK [1][-]{section.2.1}{2.1 线性回归}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.1.1}{2.1.1 直接求解}{section.2.1}% 18
\BOOKMARK [2][-]{subsection.2.1.2}{2.1.2 牛顿法（Newton's Method）}{section.2.1}% 19
\BOOKMARK [2][-]{subsection.2.1.3}{2.1.3 批处理梯度下降法（batch gradient descent）}{section.2.1}% 20
\BOOKMARK [2][-]{subsection.2.1.4}{2.1.4 随机梯度下降法（stochastic gradient descent）}{section.2.1}% 21
\BOOKMARK [1][-]{section.2.2}{2.2 局部加权线性回归（LWR）}{chapter.2}% 22
\BOOKMARK [1][-]{section.2.3}{2.3 Logistic回归}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.3.1}{2.3.1 二项逻辑斯谛回归}{section.2.3}% 24
\BOOKMARK [2][-]{subsection.2.3.2}{2.3.2 多项逻辑斯谛回归}{section.2.3}% 25
\BOOKMARK [1][-]{section.2.4}{2.4 广义线性模型}{chapter.2}% 26
\BOOKMARK [2][-]{subsection.2.4.1}{2.4.1 指数分布族（the exponential family）}{section.2.4}% 27
\BOOKMARK [2][-]{subsection.2.4.2}{2.4.2 构造GLMs}{section.2.4}% 28
\BOOKMARK [3][-]{subsubsection.2.4.2.1}{2.4.2.1 最小二乘法（Ordinary Least Square）}{subsection.2.4.2}% 29
\BOOKMARK [3][-]{subsubsection.2.4.2.2}{2.4.2.2 Logistic Regression}{subsection.2.4.2}% 30
\BOOKMARK [3][-]{subsubsection.2.4.2.3}{2.4.2.3 Softmax Regression}{subsection.2.4.2}% 31
\BOOKMARK [2][-]{subsection.2.4.3}{2.4.3 总结}{section.2.4}% 32
\BOOKMARK [0][-]{chapter.3}{ 第3章 生成算法}{}% 33
\BOOKMARK [1][-]{section.3.1}{3.1 高斯判别分析（Gaussian Discriminant Analysis）}{chapter.3}% 34
\BOOKMARK [2][-]{subsection.3.1.1}{3.1.1 多元正态分布（Multivariate Normal Distribution）}{section.3.1}% 35
\BOOKMARK [2][-]{subsection.3.1.2}{3.1.2 高斯判别分析（GDA）}{section.3.1}% 36
\BOOKMARK [2][-]{subsection.3.1.3}{3.1.3 高斯判别分析与logistic回归的对比}{section.3.1}% 37
\BOOKMARK [1][-]{section.3.2}{3.2 朴素贝叶斯法}{chapter.3}% 38
\BOOKMARK [2][-]{subsection.3.2.1}{3.2.1 贝叶斯公式}{section.3.2}% 39
\BOOKMARK [2][-]{subsection.3.2.2}{3.2.2 朴素贝叶斯法}{section.3.2}% 40
\BOOKMARK [3][-]{subsubsection.3.2.2.1}{3.2.2.1 模型}{subsection.3.2.2}% 41
\BOOKMARK [3][-]{subsubsection.3.2.2.2}{3.2.2.2 实现}{subsection.3.2.2}% 42
\BOOKMARK [0][-]{chapter.4}{ 第4章 决策树}{}% 43
\BOOKMARK [0][-]{chapter.5}{ 第5章 最大熵}{}% 44
\BOOKMARK [0][-]{chapter.6}{ 第6章 支持向量机}{}% 45
\BOOKMARK [1][-]{section.6.1}{6.1 模型}{chapter.6}% 46
\BOOKMARK [1][-]{section.6.2}{6.2 函数间隔和几何间隔（functional and geometric margins）}{chapter.6}% 47
\BOOKMARK [1][-]{section.6.3}{6.3 SVM模型}{chapter.6}% 48
\BOOKMARK [1][-]{section.6.4}{6.4 线性可分SVM}{chapter.6}% 49
\BOOKMARK [1][-]{section.6.5}{6.5 线性SVM}{chapter.6}% 50
\BOOKMARK [1][-]{section.6.6}{6.6 非线性SVM}{chapter.6}% 51
\BOOKMARK [2][-]{subsection.6.6.1}{6.6.1 非线性分类问题}{section.6.6}% 52
\BOOKMARK [2][-]{subsection.6.6.2}{6.6.2 核（Kernel）}{section.6.6}% 53
\BOOKMARK [2][-]{subsection.6.6.3}{6.6.3 常用核函数}{section.6.6}% 54
\BOOKMARK [0][-]{chapter.7}{ 第7章 提升方法}{}% 55
\BOOKMARK [0][-]{chapter.8}{ 第8章 EM方法}{}% 56
\BOOKMARK [1][-]{section.8.1}{8.1 Jensen不等式}{chapter.8}% 57
\BOOKMARK [0][-]{chapter.9}{ 第9章 隐马尔可夫模型}{}% 58
\BOOKMARK [1][-]{section.9.1}{9.1 模型}{chapter.9}% 59
\BOOKMARK [1][-]{section.9.2}{9.2 概率计算问题}{chapter.9}% 60
\BOOKMARK [2][-]{subsection.9.2.1}{9.2.1 直接计算}{section.9.2}% 61
\BOOKMARK [2][-]{subsection.9.2.2}{9.2.2 前向算法（forward algorithm）}{section.9.2}% 62
\BOOKMARK [2][-]{subsection.9.2.3}{9.2.3 后向算法（backward algorithm）}{section.9.2}% 63
\BOOKMARK [1][-]{section.9.3}{9.3 学习问题}{chapter.9}% 64
\BOOKMARK [1][-]{section.9.4}{9.4 预测问题}{chapter.9}% 65
\BOOKMARK [0][-]{chapter.10}{ 第10章 附录}{}% 66
\BOOKMARK [1][-]{section.10.1}{10.1 矩阵分析（matrix analysis）}{chapter.10}% 67
\BOOKMARK [2][-]{subsection.10.1.1}{10.1.1 迹\(Trace\)和导数\(matrix derivatives\)}{section.10.1}% 68
\BOOKMARK [1][-]{section.10.2}{10.2 常用不等式}{chapter.10}% 69
\BOOKMARK [2][-]{subsection.10.2.1}{10.2.1 柯西不等式（Cauchy Inequality）}{section.10.2}% 70
\BOOKMARK [2][-]{subsection.10.2.2}{10.2.2 赫尔德不等式（Hlder Inequality）}{section.10.2}% 71
\BOOKMARK [2][-]{subsection.10.2.3}{10.2.3 闵可夫斯基不等式（Minkowski Inequality）}{section.10.2}% 72
\BOOKMARK [0][-]{chapter*.2}{ 参考文献}{}% 73
